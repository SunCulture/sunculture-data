{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7c62350a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÇ Working directory set to: C:\\Users\\USER\\Documents\\sunculture-data\\data-science\\60_decibels-Ug(2025)\\v2\n"
     ]
    }
   ],
   "source": [
    "import importlib\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "\n",
    "#Installing and importing required packages\n",
    "\n",
    "def install_and_import_packages():\n",
    "    \"\"\"\n",
    "    Installs and imports all required packages.\n",
    "    \"\"\"\n",
    "    packages = {\n",
    "        \"pandas\": \"pd\",\n",
    "        \"numpy\": \"np\",\n",
    "        \"matplotlib.pyplot\": \"plt\",\n",
    "        \"seaborn\": \"sns\",\n",
    "        \"sklearn.preprocessing\": \"preprocessing\",\n",
    "        \"sklearn.decomposition\": \"decomposition\",\n",
    "        \"datetime\": None,\n",
    "        \"os\": None,\n",
    "        \"statsmodels\":None\n",
    "    }\n",
    "\n",
    "    for package, alias in packages.items():\n",
    "        try:\n",
    "            importlib.import_module(package)\n",
    "            print(f\"‚úÖ {package} is already installed.\")\n",
    "        except ImportError:\n",
    "            base_pkg = package.split('.')[0]  # e.g. sklearn from sklearn.preprocessing\n",
    "            print(f\"üì¶ Installing {base_pkg} ...\")\n",
    "            subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", base_pkg])\n",
    "\n",
    "    # Now import them into the global namespace\n",
    "    globals().update({\n",
    "        \"pd\": importlib.import_module(\"pandas\"),\n",
    "        \"np\": importlib.import_module(\"numpy\"),\n",
    "        \"plt\": importlib.import_module(\"matplotlib.pyplot\"),\n",
    "        \"sns\": importlib.import_module(\"seaborn\"),\n",
    "        \"StandardScaler\": importlib.import_module(\"sklearn.preprocessing\").StandardScaler,\n",
    "        \"PCA\": importlib.import_module(\"sklearn.decomposition\").PCA,\n",
    "        \"datetime\": importlib.import_module(\"datetime\"),\n",
    "        \"os\": importlib.import_module(\"os\")\n",
    "    })\n",
    "\n",
    "    print(\"\\nüéâ All required packages are installed and imported!\")\n",
    "\n",
    "\n",
    "\n",
    "# Automatically set working directory to this script's folder\n",
    "os.chdir(r\"C:\\Users\\USER\\Documents\\sunculture-data\\data-science\\60_decibels-Ug(2025)\\v2\")\n",
    "print(f\"üìÇ Working directory set to: {os.getcwd()}\")\n",
    "\n",
    "\n",
    "\n",
    "#File path to the dataset\n",
    "file_path = r\"C:\\Users\\USER\\Documents\\sunculture-data\\data-science\\60_decibels-Ug(2025)\\data.xlsx\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8cf396f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reading, exploring, and cleaning the data\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "def read_data(file_path):\n",
    "    \"\"\"\n",
    "    Reads an Excel file into a pandas DataFrame.\n",
    "    \n",
    "    Args:\n",
    "        file_path (str or Path): The path to the Excel file.\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: Loaded dataset.\n",
    "    \"\"\"\n",
    "    file_path = Path(file_path)\n",
    "    if not file_path.exists():\n",
    "        raise FileNotFoundError(f\"‚ùå File not found: {file_path}\")\n",
    "    \n",
    "    df = pd.read_excel(file_path)\n",
    "    print(f\"‚úÖ Successfully loaded file: {file_path}\")\n",
    "    print(f\"üßæ Dataset shape: {df.shape}\")\n",
    "    return df\n",
    "\n",
    "\n",
    "def explore_data(df):\n",
    "    \"\"\"\n",
    "    Provides a quick overview of the dataset.\n",
    "    \n",
    "    Args:\n",
    "        df (pd.DataFrame): DataFrame to explore.\n",
    "    \"\"\"\n",
    "    print(\"üîç First 5 rows:\")\n",
    "    display(df.head())\n",
    "\n",
    "    print(\"\\nüìä Column info:\")\n",
    "    print(df.info())\n",
    "\n",
    "    print(\"\\nüßÆ Missing values per column:\")\n",
    "    print(df.isnull().sum())\n",
    "\n",
    "    print(\"\\nüî¢ Basic statistics:\")\n",
    "    display(df.describe(include='all'))\n",
    "\n",
    "\n",
    "def clean_data(df):\n",
    "    \"\"\"\n",
    "    Cleans and processes the dataset:\n",
    "      - Keeps only required columns\n",
    "      - Converts Date_of_Birth to datetime\n",
    "      - Calculates Age\n",
    "      - Removes rows with nulls in required columns\n",
    "    \n",
    "    Args:\n",
    "        df (pd.DataFrame): Raw dataset.\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: Cleaned dataset.\n",
    "    \"\"\"\n",
    "    # Select only the required columns\n",
    "    cols_to_keep = ['Customer_Id','Sale_Date','Product','Region',\n",
    "                    'Account_type','Gender','Date_of_Birth','District','Units']\n",
    "    \n",
    "    df = df[cols_to_keep].copy()\n",
    "\n",
    "    # Convert Date_of_Birth to datetime\n",
    "    df['Date_of_Birth'] = pd.to_datetime(df['Date_of_Birth'], errors='coerce')\n",
    "\n",
    "    # Compute Age\n",
    "    current_year = datetime.datetime.now().year\n",
    "    df['Age'] = current_year - df['Date_of_Birth'].dt.year\n",
    "\n",
    "    # Drop rows with missing required fields\n",
    "    required_cols = [\"Customer_Id\", \"Sale_Date\", \"Product\", \"Region\",\n",
    "                     \"Account_type\", \"Gender\", \"District\", \"Units\"]\n",
    "    \n",
    "    df_clean = df.dropna(subset=required_cols)\n",
    "\n",
    "    print(f\"‚úÖ Cleaned data: {df_clean.shape[0]} rows remaining after removing nulls.\")\n",
    "    return df_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bad1dad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Visualising the data\n",
    "def analyze_and_visualize_sales(df_clean):\n",
    "    \"\"\"\n",
    "    Performs aggregation and visualization on the cleaned dataset.\n",
    "    Generates multiple subplots showing customer distribution,\n",
    "    units sold, and sales breakdowns by region, gender, account type, and product.\n",
    "\n",
    "    Args:\n",
    "        df_clean (pd.DataFrame): Cleaned dataset\n",
    "    \"\"\"\n",
    "    # ---- Aggregations ----\n",
    "    customers_region_gender = (\n",
    "        df_clean.groupby(['Region', 'Gender'])['Customer_Id']\n",
    "        .nunique()\n",
    "        .reset_index()\n",
    "    )\n",
    "\n",
    "    sales_per_region = df_clean.groupby('Region')['Units'].sum()\n",
    "\n",
    "    sales_per_account = df_clean.groupby('Account_type')['Units'].sum()\n",
    "    sales_pct = (sales_per_account / sales_per_account.sum()) * 100\n",
    "\n",
    "    sales_region_account = (\n",
    "        df_clean.groupby(['Region', 'Account_type'])['Units']\n",
    "        .sum()\n",
    "        .reset_index()\n",
    "    )\n",
    "\n",
    "    sales_region_product = (\n",
    "        df_clean.groupby(['Region', 'Product'])['Units']\n",
    "        .sum()\n",
    "        .reset_index()\n",
    "    )\n",
    "\n",
    "    # ---- Subplots Layout ----\n",
    "    fig, axes = plt.subplots(3, 2, figsize=(16, 14))\n",
    "\n",
    "    # Age Distribution\n",
    "    sns.histplot(\n",
    "        data=df_clean,\n",
    "        x=\"Age\",\n",
    "        hue=\"Region\",\n",
    "        element=\"step\",\n",
    "        stat=\"density\",\n",
    "        common_norm=False,\n",
    "        ax=axes[0, 0]\n",
    "    )\n",
    "    axes[0, 0].set_title('Age Distribution by Region')\n",
    "\n",
    "    # Number of Customers per Region by Gender\n",
    "    sns.barplot(\n",
    "        data=customers_region_gender,\n",
    "        x=\"Region\",\n",
    "        y=\"Customer_Id\",\n",
    "        hue=\"Gender\",\n",
    "        ax=axes[0, 1]\n",
    "    )\n",
    "    axes[0, 1].set_title('Number of Customers per Region by Gender')\n",
    "    axes[0, 1].set_ylabel('Distinct Customers')\n",
    "\n",
    "    # Add value labels\n",
    "    for p in axes[0, 1].patches:\n",
    "        height = p.get_height()\n",
    "        axes[0, 1].text(\n",
    "            p.get_x() + p.get_width() / 2.,\n",
    "            height + 0.5,\n",
    "            int(height),\n",
    "            ha=\"center\"\n",
    "        )\n",
    "\n",
    "    # Units Sold per Region\n",
    "    sales_per_region.plot(kind='bar', ax=axes[1, 0], color='orange')\n",
    "    axes[1, 0].set_title('Total Units Sold per Region')\n",
    "    axes[1, 0].set_ylabel('Units')\n",
    "    for i, v in enumerate(sales_per_region):\n",
    "        axes[1, 0].text(i, v + 0.5, str(v), ha='center')\n",
    "\n",
    "    # Percentage of Sales per Account Type (Pie Chart)\n",
    "    sales_pct.plot(kind='pie', autopct='%1.1f%%', ax=axes[1, 1])\n",
    "    axes[1, 1].set_title('Sales Percentage by Account Type')\n",
    "    axes[1, 1].set_ylabel('')\n",
    "\n",
    "    # Sales per Region by Account Type\n",
    "    sns.barplot(\n",
    "        data=sales_region_account,\n",
    "        x=\"Region\",\n",
    "        y=\"Units\",\n",
    "        hue=\"Account_type\",\n",
    "        ax=axes[2, 0]\n",
    "    )\n",
    "    axes[2, 0].set_title('Sales per Region by Account Type')\n",
    "    axes[2, 0].set_ylabel('Units')\n",
    "\n",
    "    # Sales per Region by Product\n",
    "    sns.barplot(\n",
    "        data=sales_region_product,\n",
    "        x=\"Region\",\n",
    "        y=\"Units\",\n",
    "        hue=\"Product\",\n",
    "        ax=axes[2, 1]\n",
    "    )\n",
    "    axes[2, 1].set_title('Sales per Region by Product')\n",
    "    axes[2, 1].set_ylabel('Units')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    print(\"‚úÖ Visualization complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0eef65bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using Median to replace null values in the Age column\n",
    "def fill_age_with_group_median(df_clean, group_col='District', target_col='Age'):\n",
    "    \"\"\"\n",
    "    Fills missing values in the target column (e.g. 'Age') \n",
    "    using the median value of that column within each group (e.g. per 'District').\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): Input DataFrame.\n",
    "        group_col (str): Column name to group by (default 'District').\n",
    "        target_col (str): Column whose nulls will be filled (default 'Age').\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame with filled values in the target column.\n",
    "    \"\"\"\n",
    "    # Make a copy to avoid modifying the original DataFrame directly\n",
    "    df = df_clean.copy()\n",
    "\n",
    "    # Fill missing values using median per group\n",
    "    df[target_col] = df.groupby(group_col)[target_col].transform(\n",
    "        lambda x: x.fillna(x.median())\n",
    "    )\n",
    "\n",
    "    print(f\"‚úÖ Missing '{target_col}' values filled using median by '{group_col}'.\")\n",
    "    return df\n",
    "\n",
    "\n",
    "#Dropping rows with missing Age values\n",
    "def drop_missing_age(df, age_col='Age'):\n",
    "    \"\"\"\n",
    "    Drops rows where the Age column is null.\n",
    "    \n",
    "    Args:\n",
    "        df (pd.DataFrame): Input DataFrame.\n",
    "        age_col (str): Column name for age (default 'Age').\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Cleaned DataFrame with missing ages removed.\n",
    "    \"\"\"\n",
    "    initial_rows = len(df)\n",
    "    df_final = df.dropna(subset=[age_col]).copy()\n",
    "    removed = initial_rows - len(df_final)\n",
    "\n",
    "    print(f\"‚úÖ Dropped {removed} rows with missing '{age_col}'.\")\n",
    "    print(f\"Remaining rows: {len(df_final)}\")\n",
    "    return df_final\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2c60c49d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bayesian total sample size + proportional stratified allocation\n",
    "\n",
    "from scipy.stats import beta as sp_beta\n",
    "\n",
    "# ---------- 1. Posterior Width ----------\n",
    "def beta_posterior_hdi_width(k, n, alpha=1, beta_param=1, hdi_prob=0.95):\n",
    "    \"\"\"\n",
    "    Computes the 95% HDI (highest density interval) width \n",
    "    for a Beta-Binomial posterior given k successes in n trials.\n",
    "    \"\"\"\n",
    "    a_post = alpha + k\n",
    "    b_post = beta_param + n - k\n",
    "    lower = sp_beta.ppf((1 - hdi_prob) / 2, a_post, b_post)\n",
    "    upper = sp_beta.ppf(1 - (1 - hdi_prob) / 2, a_post, b_post)\n",
    "    return upper - lower, (lower, upper)\n",
    "\n",
    "# ---------- 2. Bayesian Sample Size Estimation ----------\n",
    "def find_min_n_bayesian(p_assumed, target_width=0.10, max_n=5000, step=50,\n",
    "                        alpha_prior=1, beta_prior=1, verbose=False):\n",
    "    \"\"\"\n",
    "    Iteratively finds the minimal n such that the 95% posterior interval width\n",
    "    is less than or equal to target_width.\n",
    "    \"\"\"\n",
    "    for n in range(step, max_n + 1, step):\n",
    "        k = int(round(p_assumed * n))\n",
    "        width, ci = beta_posterior_hdi_width(\n",
    "            k, n, alpha=alpha_prior, beta_param=beta_prior, hdi_prob=0.95\n",
    "        )\n",
    "        if verbose:\n",
    "            print(f\"n={n:5d}  k={k:4d}  CI width={width:.4f}  95% CI=({ci[0]:.4f},{ci[1]:.4f})\")\n",
    "        if width <= target_width:\n",
    "            return n, k, width, ci\n",
    "    return None, None, None, None\n",
    "\n",
    "# ---------- 3. Stratified Proportional Allocation ----------\n",
    "def stratified_allocation(frame, strata_cols, min_n, min_per_stratum=5):\n",
    "    \"\"\"\n",
    "    Allocates a total sample size across strata proportionally to customer counts.\n",
    "    Ensures a minimum number of samples per stratum and corrects rounding differences.\n",
    "    \"\"\"\n",
    "    available_strata = [c for c in strata_cols if c in frame.columns]\n",
    "    if not available_strata:\n",
    "        raise RuntimeError(\"‚ùå No strata columns available in the frame.\")\n",
    "\n",
    "    strata_ser = frame.groupby(available_strata)['Customer_Id'].nunique()\n",
    "    total_customers = strata_ser.sum()\n",
    "\n",
    "    alloc = (strata_ser / total_customers * min_n).round().astype(int)\n",
    "\n",
    "    # Fix rounding mismatch\n",
    "    diff = int(min_n - alloc.sum())\n",
    "    if diff != 0:\n",
    "        idx = 0\n",
    "        keys = list(alloc.index)\n",
    "        while diff != 0:\n",
    "            key = keys[idx % len(keys)]\n",
    "            alloc.loc[key] += 1 if diff > 0 else -1\n",
    "            diff = int(min_n - alloc.sum())\n",
    "            idx += 1\n",
    "\n",
    "    # Enforce minimum per stratum\n",
    "    alloc = alloc.apply(lambda x: max(x, min_per_stratum))\n",
    "\n",
    "    # Adjust downward if sum exceeds min_n\n",
    "    while alloc.sum() > min_n:\n",
    "        reducible = alloc[alloc > min_per_stratum].sort_values(ascending=False)\n",
    "        if reducible.empty:\n",
    "            break\n",
    "        top = reducible.index[0]\n",
    "        alloc.loc[top] -= 1\n",
    "\n",
    "    alloc_df = alloc.reset_index()\n",
    "    alloc_df.columns = available_strata + ['n_alloc']\n",
    "\n",
    "    return alloc_df\n",
    "\n",
    "# ---------- 4. Main Driver Function ----------\n",
    "def bayesian_sample_planner(df_final, \n",
    "                            IndicatorCol=None, \n",
    "                            target_width=0.10, \n",
    "                            max_n=1854, \n",
    "                            step=50,\n",
    "                            prior_alpha=1, \n",
    "                            prior_beta=1, \n",
    "                            strata_cols=None, \n",
    "                            min_per_stratum=5,\n",
    "                            random_seed=2025, \n",
    "                            output_csv=\"bayes_stratified_allocation.csv\",\n",
    "                            verbose=False):\n",
    "    \"\"\"\n",
    "    Full Bayesian sample size planning workflow:\n",
    "      1. Detect indicator variable\n",
    "      2. Estimate minimal Bayesian sample size\n",
    "      3. Perform stratified proportional allocation\n",
    "      4. Save results to CSV\n",
    "    \"\"\"\n",
    "    np.random.seed(random_seed)\n",
    "\n",
    "    # Copy DataFrame\n",
    "    frame = df_final.copy()\n",
    "\n",
    "    # ---- Indicator detection ----\n",
    "    if IndicatorCol is None:\n",
    "        if 'Units' in frame.columns:\n",
    "            frame['_indicator'] = (frame['Units'].fillna(0) > 0).astype(int)\n",
    "            IndicatorCol = '_indicator'\n",
    "            if verbose: print(\"Auto-selected indicator: Units > 0\")\n",
    "        else:\n",
    "            frame['_indicator'] = frame['Age'].notnull().astype(int)\n",
    "            IndicatorCol = '_indicator'\n",
    "            if verbose: print(\"Auto-selected indicator: Age.notnull() (fallback).\")\n",
    "    else:\n",
    "        if IndicatorCol not in frame.columns:\n",
    "            raise ValueError(f\"IndicatorCol '{IndicatorCol}' not found in dataframe.\")\n",
    "        if frame[IndicatorCol].nunique() > 2:\n",
    "            if frame[IndicatorCol].dtype.kind in 'biufc':\n",
    "                frame['_indicator'] = (frame[IndicatorCol].fillna(0) > 0).astype(int)\n",
    "                IndicatorCol = '_indicator'\n",
    "            else:\n",
    "                raise ValueError(\"Indicator column not binary.\")\n",
    "        else:\n",
    "            vals = sorted(frame[IndicatorCol].dropna().unique())\n",
    "            if set(vals) <= {0,1}:\n",
    "                frame['_indicator'] = frame[IndicatorCol].astype(int)\n",
    "                IndicatorCol = '_indicator'\n",
    "            else:\n",
    "                mapping = {vals[0]: 0, vals[-1]: 1}\n",
    "                frame['_indicator'] = frame[IndicatorCol].map(mapping).astype(int)\n",
    "                IndicatorCol = '_indicator'\n",
    "                if verbose: print(f\"Mapped {vals} to {mapping}.\")\n",
    "\n",
    "    # ---- Empirical baseline ----\n",
    "    observed_p = frame[IndicatorCol].mean()\n",
    "    N_pop = frame['Customer_Id'].nunique()\n",
    "    print(f\"Frame size (unique customers): {N_pop}, observed pÃÇ = {observed_p:.4f}\")\n",
    "\n",
    "    # ---- Bayesian sample size ----\n",
    "    min_n, min_k, width, ci = find_min_n_bayesian(\n",
    "        observed_p, target_width, max_n, step,\n",
    "        alpha_prior=prior_alpha, beta_prior=prior_beta, verbose=verbose\n",
    "    )\n",
    "    if min_n is None:\n",
    "        raise RuntimeError(\"‚ùå Could not find n that meets target_width. Increase max_n or relax target_width.\")\n",
    "    print(f\"‚úÖ Suggested n = {min_n}, 95% CI width = {width:.4f}, CI = ({ci[0]:.4f},{ci[1]:.4f})\")\n",
    "\n",
    "    # ---- Stratified allocation ----\n",
    "    if strata_cols is None:\n",
    "        strata_cols = ['Region', 'Gender', 'Account_type', 'Product']\n",
    "\n",
    "    alloc_df = stratified_allocation(frame, strata_cols, min_n, min_per_stratum)\n",
    "    print(f\"\\nüìä Stratified allocation (first 10 rows):\\n{alloc_df.head(10)}\")\n",
    "    print(f\"Total allocated = {alloc_df['n_alloc'].sum()}\")\n",
    "\n",
    "    alloc_df.to_csv(output_csv, index=False)\n",
    "    print(f\"‚úÖ Allocation saved to '{output_csv}'\")\n",
    "\n",
    "    return alloc_df\n",
    "\n",
    "\n",
    "# ---------- 5. Sample Selection Function ----------\n",
    "def select_stratified_sample(frame, alloc_df, strata_cols, random_seed=2025,\n",
    "                             output_excel=\"bayes_selected_sample.xlsx\"):\n",
    "    \"\"\"\n",
    "    Selects a stratified random sample based on an allocation table.\n",
    "\n",
    "    Steps:\n",
    "      1. Count available customers per stratum\n",
    "      2. Merge counts into allocation table\n",
    "      3. Cap allocations if they exceed available customers\n",
    "      4. Randomly sample per stratum (without replacement)\n",
    "      5. Combine and export the final sample to Excel\n",
    "\n",
    "    Args:\n",
    "        frame (pd.DataFrame): Full dataset (with 'Customer_Id' column)\n",
    "        alloc_df (pd.DataFrame): Allocation table containing strata and n_alloc\n",
    "        strata_cols (list): Columns to use for stratification\n",
    "        random_seed (int): Random seed for reproducibility\n",
    "        output_excel (str): File path to save the final sampled dataset\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Final sampled dataset\n",
    "    \"\"\"\n",
    "    # Step 1: Count available customers per stratum\n",
    "    strata_counts = (\n",
    "        frame.groupby(strata_cols)['Customer_Id']\n",
    "        .nunique()\n",
    "        .reset_index(name='N_available')\n",
    "    )\n",
    "\n",
    "    # Step 2: Merge available counts into allocation table\n",
    "    alloc_df = alloc_df.merge(strata_counts, on=strata_cols, how='left')\n",
    "\n",
    "    # Step 3: Cap allocations so they do not exceed available customers\n",
    "    alloc_df['n_alloc'] = alloc_df[['n_alloc', 'N_available']].min(axis=1).astype(int)\n",
    "\n",
    "    print(\"\\nüìä Adjusted stratified allocation (first 10 rows):\")\n",
    "    print(alloc_df.head(10))\n",
    "    print(f\"\\nAdjusted total sample size = {alloc_df['n_alloc'].sum()}\")\n",
    "\n",
    "    # Step 4: Select samples per stratum\n",
    "    selected_samples = []\n",
    "    for _, row in alloc_df.iterrows():\n",
    "        subset = frame.copy()\n",
    "        for col in strata_cols:\n",
    "            subset = subset[subset[col] == row[col]]\n",
    "\n",
    "        # Sample without replacement\n",
    "        if len(subset) >= row['n_alloc']:\n",
    "            sample = subset.sample(n=row['n_alloc'], random_state=random_seed)\n",
    "        else:\n",
    "            sample = subset  # if fewer available, take all\n",
    "        selected_samples.append(sample)\n",
    "\n",
    "    # Step 5: Combine all selected strata samples\n",
    "    final_sample = pd.concat(selected_samples, ignore_index=True)\n",
    "\n",
    "    # Save to Excel\n",
    "    final_sample.to_excel(output_excel, index=False)\n",
    "\n",
    "    print(f\"\\n‚úÖ Final selected sample size = {final_sample['Customer_Id'].nunique()} unique customers\")\n",
    "    print(f\"üíæ Saved to '{output_excel}'\")\n",
    "\n",
    "    return final_sample"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
